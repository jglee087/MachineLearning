### 3. 파이토치로 구현하는 ANN



파이토치는 기본적인 수학 계산용 라이브러리를 바탕으로 그 위에 딥러닝에 필요한 그래프 형태 계산 방식을 추가한 라이브러리이다. 파이토치가 개발자에게 편리하도록 설계되었더라도 행렬 계산이 많은 딥러닝의 특성 때문에 파이토치의 자료구조는 파이썬과 다르다. 가장 기본적인 자료구조인 텐서가 대표적이다.



### 3.1 텐서와 Autograd

#### 3.1.1 텐서 자유자재로 다루기

텐서는 파이토치에서 다양한 수식을 계산하는 데 사용하는 가장 기본적인 자료구조이다. 수학의 벡터나 행렬을 일반화한 개념으로서, 숫자들을 특정한 모양으로 배열한 것이다. 텐서에는 랭크라는 개념이 있다. 랭크가 0이면 스칼라, 1이면 벡터, 2이면 행렬, 3이상은 일반적으로 랭크 n 텐서라고 한다.

- 1 -> 스칼라이고 모양은 ()
- [1,2,3] -> 벡터, 모양은 (3,)
- [[1,2,3]] -> 행렬, 모양은 (1,3)



- 텐서

```python
import torch

x=torch.tensor([ [1,2,3,], [4,5,6,], [7,8,9,]])
print(x)
print("Size:",x.size())
print("Shape:",x.shape)
print("Rank:",x.ndimension())
```

```
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
Size: torch.Size([3, 3])
Shape: torch.Size([3, 3])
Rank: 2
```

텐서의 형태를 살펴보면 3x3 행렬이고 size는 ([3,3])이고 shape은 ([3,3])이고 랭크는 2이다.



- unsqueeze() 함수

```python
x = torch.unsqueeze(x,0)
print(x)
print("Size:",x.size())
print("Shape:",x.shape)
print("Rank:",x.ndimension())
```

```
tensor([[[1, 2, 3],
         [4, 5, 6],
         [7, 8, 9]]])
Size: torch.Size([1, 3, 3])
Shape: torch.Size([1, 3, 3])
Rank: 3
```

이 코드는 [3,3] 형태의 랭크 2 텐서의 첫 번째 자리에 1이라는 차원 값을 추가해 [1,3,3] 모양의 랭크 3 텐서로 변경한다. 랭크가 늘어나도 텐서 속 원소의 수는 유지된다.



- squeeze()함수

```python
x= torch.squeeze(x)
print(x)
print("Size:",x.size())
print("Shape:",x.shape)
print("Rank:",x.ndimension())
```

```
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
Size: torch.Size([3, 3])
Shape: torch.Size([3, 3])
Rank: 2
```

이 함수는 텐서의 랭크 중 크기가 1인 랭크를 삭제하여 다시 랭크 2 텐서로 되돌리는 함수이다.



view()함수

```python
x=x.view(9)
print(x)
print("Size:",x.size())
print("Shape:",x.shape)
print("Rank:",x.ndimension())
```

```
tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
Size: torch.Size([9])
Shape: torch.Size([9])
Rank: 1
```

이 함수는 랭크 2의 [3,3] 모양인 x를 랭크 1의 [9] 모양으로 바꾸어주는 함수이다.



**이와 같이 함수들은 텐서의 원소 수를 그대로 유지하면서 모양과 차원을 조절한다.**



#### 3.1.2 텐서를 이용한 연산과 행렬곱

행렬은 랭크 2 텐서와 같은 개념이고, 숫자들을 정사각형형태로 배치한 배열이다. 행렬의 높이를 행, 너비를 열이라고 한다. 행렬에 곱셈하려면 반드시 한가지 조건이 만족해야 한다. 행렬 A, B가 존재할 때, 두 행렬의 곱셈이 정의되려면 행렬 A의 열 개수와 행렬 B의 행 개수가 일치해야만 행렬의 곱셈이 가능하다. 그리고 행렬 A와 B의 곱셈의 크기는 행렬 A의 행과 행렬 B의 열 개수로 결정된다. 예를 들어, 행렬 A가 3x4 행렬이고 행렬 B가 4x2 행렬이면, 행렬 곱을 하면 결과는 3x2 행렬이 된다.



```python
import torch

w=torch.rand(4,2, dtype=torch.float)
x=torch.tensor([[1.,2.], [3.,4.]])
print("w size:", w.size())
print("x size:", x.size())
print("w:", w)
print("x:", x)
```

```
w size: torch.Size([4, 2])
x size: torch.Size([2, 2])
w: tensor([[0.6822, 0.9607],
        [0.1278, 0.3265],
        [0.4398, 0.1592],
        [0.7279, 0.1204]])
x: tensor([[1., 2.],
        [3., 4.]])
```



행렬곱 이외에 행렬 덧셈 연산을 위해 b라는 텐서를  정의해보자.

```python
b= torch.randn(4,2, dtype=torch.float)
print("b size:",b.size())
print("b:", b)
```

```
b size: torch.Size([4, 2])
b: tensor([[ 0.8803, -1.3402],
        [ 1.3077, -0.9235],
        [-1.6405,  0.1975],
        [ 0.2409, -0.6500]])
```



행렬곱은 **torch.mm()**함수를 사용한다.

```python
wx = torch.mm(w,x)
print("wx size:", wx.size())
print("wx:", wx)
```

```
wx size: torch.Size([4, 2])
wx: tensor([[3.5642, 5.2070],
        [1.1072, 1.5615],
        [0.9175, 1.5165],
        [1.0891, 1.9374]])
```



그 다음 행렬의 덧셈을 해보자. 행렬의 덧셈을 하려면 조건은 더하고자 하는 행렬 A, B와 크기가 같아야만 행렬의 덧셈이 가능하다.

```python
res = wx+b
print("res size:",res.size())
print("res:", res)
```

```
res size: torch.Size([4, 2])
res: tensor([[ 4.4445,  3.8668],
        [ 2.4149,  0.6380],
        [-0.7230,  1.7140],
        [ 1.3300,  1.2873]])
```



#### 3.1.3 Autograd

머신러닝의 일반적인 학습 방법을 알아보자. 머신러닝 모델은 입력된 데이터를 기반으로 학습한다. 아직 충분한 데이터를 보지 못해서 학습이 끝나지 않은 모델은 정답이 아닌 결과를 출력할 가능성이 크다. 이처럼 데이터에 대한 정답과 머신러닝 모델이 예측한 결과의 차이를 산술적으로 표현한 것을 **거리(Distance)**라고 한다. 그리고 학습 데이터로 계산한 거리들의 평균을 **오차(Loss)**라고 한다. 오차가 작을수록 주어진 데이터에 대해 더 정확한 답을 낸다고 볼 수 있다.

오차를 최소화하는 데는 여러 알고리즘이 쓰이지만, 그 중 하나는 **경사하강법(gradient descent)**이다. 경사하강법이란, 오차를 수학 함수로 표현한 후 미분하여 이 함수의 기울기를 구해 오차의 최솟값이 있는 방향을 찾아내는 알고리즘이다. 복잡한 인공 신경망 모델에서는 많은 수의 계산을 해주어야 한다. 파이토치에는 Autograd라는 미분 계산을 해주는 것이 있다.

```python
import torch

w = torch.tensor(1.0 , requires_grad=True)
a = 3*w
l = a**2
l.backward()
print('l을 w로 미분한 값은 {}'.format(w.grad))
```

수식으로 표현하면 다음과 같다.

$l= a^2=(3w)^2=9w^2$

이 l을 w로 미분하려면 연쇄법칙을 이용하여 a와 w로 차례대로 미분해야 하는데, 이때 backward() 함수를 사용하면 된다.



### 3.2 경사하강법으로 이미지 복원하기

#### 3.2.1 오염된 이미지 문제

이미지 처리를 위해 만들어 두었던 weird_function() 함수에 실수로 100x100 픽셀의 오염된 이미지가 만들어졌다. 이 오염된 이미지와 오염되기 전 원본 이미지를 동시에 파일로 저장하려고 했으나, 모종의 이유로 원본 이미지 파일은 삭제된 상황이다. 다행히도 weird_function() 함수의 소스코드가 남아 있다. 오염된 이미지와 weird_function() 함수를 활용해 원본 이미지를 복원해 보자.



#### 3.2.2 오염된 이미지를 복원하는 방법

대부분의 프로그래머는 다음과 같이 문제에 접근할 것이다.

1. weird_function() 함수의 소스코드를 분석한다.
2. 분석을 토대로 weird_function() 함수의 동작을 반대로 이행하는 함수를 구현한다.
3. 2에서 구현한 함수에 오염된 이미지를 입력해서 복구된 이미지를 출력한다.

하지만 weird_function() 함수의 모든 동작을 되돌리려면 모든 동작을 세밀하게 파악해야 한다. 코드에 관한 자세한 설명이나 사전지식이 없으면 시간이 오래 걸리는 까다로운 작업이다.



이런 방식 대신에 머신러닝과 수학적 최적화에 가까에 방법을 택할 것이다. 그 때에 사고 과정은 다음과 같다.

1. 오염된 이미지와 크기가 같은 랜덤 텐서를 생성한다.

2. 랜덤 텐서를 weird_function()에 함수에 입력해 똑같이 오염된 이미지를 가설이라고 한다.

   a. 원본 이미지가 weird_funcion() 함수에 입력되어 오염된 이미지를 출력한다.

   b. 인위적으로 생성한 무작위 이미지가 weird_function() 함수에 입력되어 가설을 출력한다.

3. 가설과 오염된 이미지가 같다면, 무작위 이미지와 원본 이미지도 같은 것이다.
4. 그러므로 weird_function(random_tensor) = broken_image 관계가 성립하도록 만든다.



자세한 설명은 3-2 Image Restoration with Gradient Descent.ipynb 파일을 참고하자.



### 3.3 신경망 모델 구현하기

#### 3.3.1 인공 신경망(ANN)

인공 신경망(Artificial Neural Network, ANN)은 인간의 뇌 혹은 신경계의 작동 방식에서 영감을 받았지만 생물학적인 구조를 그대로 따르지는 않는다.

인공 신경망에서 자극을 입력받는 **입력층(input layer)**이라고 한다. 입력층을 거친 **은닉층(hidden layer)**(혹은 중간층)을 지나, 마지막으로 **출력층(output layer**으로 전달된다. 각 층에 존재하는 한 단위의 인공뉴런을 노드라고 한다.



인공신경망의 각 노드는 입력된 신호에 대해 특정한 수학 연산을 실행한다. 각 층에 존재하는 매개변수인 가중치에 행렬곱을 하고 편향을 더해주는 것이다. **가중치**는 입력 신호가 출력에 주는 영향을 계산하는 매개변수이고, **편향**은 각 노드가 얼마나 데이터에 민감한지 알려주는 매개변수이다.

그리고 이 행렬곱의 결과는 **활성화 함수**를 거쳐 인공뉴런의 결과값을 산출하게 된다. 활성화 함수는 입력에 적절한 처리를 해서 출력 신호로 변환하는 함수이다. 이 값들은 다음 은닉층의 인공뉴런으로 전달되고, 또 가중치 곱과 활성화 함수를 거치게 되어 이런 과정을 여러 번 거친 후에 마지막 출력층에서 결과값을 만들어내는 것이 인공 신경망의 작동 원리이다. 

그렇게 나온 결과값과 정답을 비교해 오차를 계산한다. 이 오차를 기반으로 신경망 전체를 학습시키려면 출력층의 가중치부터 입력층의 가중치까지 모두 경사하강법을 활용해 변경해주어야 한다. 이렇게 쌓인 가중치를 뒤에서부터 차례대로 조정하고 최적화하는 알고리즘이 **역전파(backprogation)**알고리즘이다.



#### 3.2.2 간단한 분류 모델 구현하기

분류의 목적은 '정답'을 맞추는 것이다. 기본적인 분류에 사용할 0과 1로 된 정답을 가진 비교적 단순한 데이터셋을 생성해보고, 신경망 모델을 학습하여 입력된 데이터를 0과 1중 알맞은 카테고리로 분류하자.



자세한 설명은 3-3 Neural Network Model.ipynb 파일을 참고하자.

